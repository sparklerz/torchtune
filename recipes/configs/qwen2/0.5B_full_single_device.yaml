# Config for single device full finetuning in full_finetune_single_device.py
# using a Qwen2 0.5B
#
# This config assumes that you've run the following command before launching
# this run:
#   tune download Qwen/Qwen2-0.5B-Instruct --output-dir /tmp/Qwen2-0.5B-Instruct --ignore-patterns None
#
# To launch on a single device, run the following command from root:
#   tune run full_finetune_single_device --config qwen2/0.5B_full_single_device
#
# You can add specific overrides through the command line. For example
# to override the checkpointer directory while launching training
# you can run:
#   tune run full_finetune_single_device --config qwen2/0.5B_full_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
#
# This config works only for training on single device.

# Tokenizer
tokenizer:
  _component_: torchtune.models.qwen2.qwen2_tokenizer
  path: /tmp/Qwen2-0.5B-Instruct/vocab.json
  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt
  max_seq_len: null

# # Dataset
# dataset:
#   packed: False # Set to true for great speed ups
#   _component_: torchtune.datasets.alpaca_cleaned_dataset
# seed: null
# shuffle: False

# Dataset
dataset:
  _component_: torchtune.datasets.huggingface_dataset.HuggingFaceDataset
  path: ash001/arxiv-abstract
  train_test_split: 0.8
  start_index: 0
  end_index: 200000
packed: False
seed: 42                # seed for reproducibility
shuffle: True           # enable shuffling

# # Dataset - 3.5 sonnet
# dataset:
#   packed: False # Set to true for great speed ups
#   _component_: torchtune.datasets.huggingface_dataset
#   path: ash001/arxiv-abstract
#   train_test_split: 0.8  # 20% test set
# seed: null
# shuffle: True
# start_index: 0
# end_index: 100000

# Model Arguments
model:
  _component_: torchtune.models.qwen2.qwen2_0_5b

checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct
  checkpoint_files_initial: [
    model.safetensors
  ]
  checkpoint_files:
    - qwen2_0.5B_0-200000-epoch-1-sync-index-3-part-0001.pt
  recipe_checkpoint: null
  output_dir: /tmp/Qwen2-0.5B-Instruct-finetune
  model_type: QWEN2
resume_from_checkpoint: True
number_of_syncs_completed: 3

repo_id: "ash001/hivemind-torchtune-0.5b"

# Fine-tuning arguments
batch_size: 32
number_of_syncs_per_epoch: 4
retrain_samples_percentage: 0.3
epochs: 2
optimizer:
  _component_: torch.optim.AdamW
  fused: True
  lr: 2e-5

loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
optimizer_in_bwd: False

max_steps_per_epoch: null
gradient_accumulation_steps: 1
compile: False

# Training environment
device: cuda

# Memory management
enable_activation_checkpointing: True

# Reduced precision
dtype: bf16

# Logging
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: ${output_dir}
output_dir: /tmp/Qwen2-0.5B-Instruct-finetune
log_every_n_steps: 1
log_peak_memory_stats: True
